SEC EDGAR Financials Warehouse

A production-ready data warehouse that ingests, transforms, and visualizes SEC EDGAR company financials. Built with BigQuery, dbt, and automated data-quality checks.

ğŸ¯ Project Overview

Status: âœ… Operational Â· dbt tests: 14/14 Â· GE: 100%
Partition/Cluster: period_end_date / (cik, concept)
Automation: GitHub Actions (daily 06:00 UTC) (Cloud Run job optional)
Visualization: sec_viz dataset powering Looker Studio

Key Features

Daily pipeline: GitHub Actions runs ingestion â†’ dbt builds â†’ viz refresh

Quality gates: dbt tests + Great Expectations + smoke checks

Cost control: Partition pruning + clustering; bytes-scanned proof queries

Dashboard-ready: Pre-aggregated sec_viz tables (TTM, latest KPIs)

Ops-ready: Dockerized jobs, logs, and runbooks

ğŸ—ï¸ Architecture
SEC EDGAR API â†’ GCS â†’ BigQuery (sec*raw) â†’ dbt â†’ BigQuery (sec_curated*\*) â†’ BigQuery (sec_viz) â†’ Looker Studio

Data Flow

Ingestion â€” Python with SEC rate limiting â†’ GCS

Load â€” NDJSON â†’ BigQuery sec_raw

Transform â€” dbt â†’ sec*curated*\* facts/dims

Validate â€” GE + dbt tests

Viz â€” sec_viz tables for BI

ğŸ“Š Datasets
Layer Dataset Purpose Key Tables
Raw sec_raw Source landings raw_companyfacts, raw_submissions
Curated sec_curated_sec_curated Business logic dim_company, fct_financials_quarterly
Visualization sec_viz Dashboard-ready kpi_company_latest, kpi_ttm_revenue, company_dim

Notes

Facts partitioned by period_end_date and clustered by cik (and concept upstream).

sec_viz is optimized for BI; row counts vary by data window.

ğŸš€ Quick Start
Prereqs

GCP project with BigQuery, GCS (and Cloud Run if used)

Python 3.9+, dbt-bigquery, Great Expectations

GitHub repo with SA_JSON secret (service account key JSON)

Local Dev

# 1) Env

cp .env.example .env # set PROJECT_ID, DATASETs, BUCKET

# 2) Run pipeline

make setup && make ingest && make load && make dbt-build && make ge

# 3) Smoke test

bash tools/smoke_check.sh

CI/CD

# Deploy Cloud Run (optional)

bash deploy/cloud_run_deploy.sh

# GitHub Actions

# .github/workflows/schedule.yml â€” runs daily at 06:00 UTC

ğŸ“ˆ Visualization Layer (sec_viz)

kpi_company_latest â€” Latest period with revenue per company (KPIs + margins)

kpi_ttm_revenue â€” TTM revenue (rolling 4 quarters), ticker filled via dim_company

company_dim â€” Ticker/name metadata for labels/filters

Looker Studio tips

Data source â†’ BigQuery â†’ set Processing location = US, Credentials = Ownerâ€™s credentials, Data freshness = 15â€“60 min

Time series: Dimension period_end_date, Breakdown ticker, Metric ttm_revenue (or ttm_revenue/1e9 as â€œTTM Revenue ($B)â€)

ğŸ”§ Operations

Scheduler: GitHub Actions daily (06:00 UTC)

Manual:

export DBT_PROFILES_DIR=./dbt
dbt build --select marts_viz # build only viz tables
make all # end-to-end locally
bash tools/smoke_check.sh # infra + data smoke test

ğŸ“‹ Data Quality

dbt: not_null/unique/relationships on core columns

GE: rule suites for schema + value ranges

Monitoring: smoke script checks dataset health and viz row counts

ğŸ’° Cost & Performance

Always filter by date (hits the period_end_date partition); add ticker for clustering benefits.

Sample bytes-scanned proof:

-- Unfiltered (larger)
SELECT SUM(ttm_revenue) FROM `sec-edgar-financials-warehouse.sec_viz.kpi_ttm_revenue`;

-- Partition-pruned (smaller)
SELECT SUM(ttm_revenue)
FROM `sec-edgar-financials-warehouse.sec_viz.kpi_ttm_revenue`
WHERE period_end_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR);

ğŸ“š Docs

docs/ARCHITECTURE.md â€” design & components

docs/DEPLOYMENT.md â€” setup & CI/CD

docs/DATA_MODELS.md â€” dbt model notes

docs/VISUALIZATION.md â€” dashboard how-to

ğŸ› ï¸ Dev Structure
sec-edgar-warehouse/
â”œâ”€ src/ # ingestion/load
â”œâ”€ dbt/ # models, tests, macros
â”œâ”€ tools/ # smoke_check.sh, utilities
â”œâ”€ deploy/ # Cloud Run, infra scripts
â””â”€ .github/workflows/ # CI/CD

ğŸ” Security

Least-privilege service account; no secrets in logs

Public data only (no PII)

Audit via Cloud Logging + dbt/GE artifacts

ğŸ†˜ Troubleshooting

Auth: verify SA_JSON secret & dataset IAM

BigQuery costs: ensure date filters on fact/viz queries

Data gaps: run tools/smoke_check.sh and dbt tests

Built with: Python Â· dbt Â· BigQuery Â· GitHub Actions Â· Great Expectations Â· Looker Studio
